{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGFTkuRvzWqc"
   },
   "outputs": [],
   "source": [
    "!pip install \"tensorflow-text==2.8.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPJ9J7iPUchc"
   },
   "outputs": [],
   "source": [
    "use_builtins = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqFqKi4fqN9X"
   },
   "outputs": [],
   "source": [
    "#@title Shape checker\n",
    "class ShapeChecker():\n",
    "    def __init__(self):\n",
    "        # Keep a cache of every axis-name seen\n",
    "        self.shapes = {}\n",
    "\n",
    "    def __call__(self, tensor, names, broadcast=False):\n",
    "        if not tf.executing_eagerly():\n",
    "            return\n",
    "\n",
    "        if isinstance(names, str):\n",
    "            names = (names,)\n",
    "\n",
    "        shape = tf.shape(tensor)\n",
    "        rank = tf.rank(tensor)\n",
    "\n",
    "        if rank != len(names):\n",
    "            raise ValueError(f'Rank mismatch:\\n'\n",
    "                           f'    found {rank}: {shape.numpy()}\\n'\n",
    "                           f'    expected {len(names)}: {names}\\n')\n",
    "\n",
    "        for i, name in enumerate(names):\n",
    "            if isinstance(name, int):\n",
    "                old_dim = name\n",
    "            else:\n",
    "                old_dim = self.shapes.get(name, None)\n",
    "            new_dim = shape[i]\n",
    "\n",
    "            if (broadcast and new_dim == 1):\n",
    "                continue\n",
    "\n",
    "            if old_dim is None:\n",
    "                # If the axis name is new, add its length to the cache.\n",
    "                self.shapes[name] = new_dim\n",
    "                continue\n",
    "\n",
    "            if new_dim != old_dim:\n",
    "                raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                             f\"    found: {new_dim}\\n\"\n",
    "                             f\"    expected: {old_dim}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNpblqiNnOFu"
   },
   "outputs": [],
   "source": [
    "targ = [] \n",
    "inp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "def load_data(src, tgt):\n",
    "    with open(src, encoding = \"unicode_escape\") as src_file:\n",
    "        for cur_line in src_file:\n",
    "            cur_line = cur_line.replace(\"\\n\", \" \").strip()\n",
    "            inp.append(cur_line)\n",
    "            \n",
    "    with open(tgt, encoding = \"unicode_escape\") as tgt_file:\n",
    "        for cur_line in tgt_file:\n",
    "            cur_line = cur_line.replace(\",\", \" \").replace(\"\\n\", \" \").strip()\n",
    "            targ.append(cur_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTbSbBz55QtF"
   },
   "outputs": [],
   "source": [
    "load_data(\"sentencesTrain.txt\", \"tokensTrain.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qc6-NK1GtWQt"
   },
   "outputs": [],
   "source": [
    "for example_input_batch, example_target_batch in dataset.take(1):\n",
    "    print(example_input_batch[:5])\n",
    "    print()\n",
    "    print(example_target_batch[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chTF5N885F0P"
   },
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    # Split accecented characters.\n",
    "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "    text = tf.strings.lower(text)\n",
    "    # Keep space, a to z, and select punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "    # Add spaces around punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "    # Strip whitespace.\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZRJusVpY4Y3"
   },
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct_custom(text):\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 7000\n",
    "\n",
    "input_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmsI1Yql8FYe"
   },
   "outputs": [],
   "source": [
    "input_text_processor.adapt(inp)\n",
    "\n",
    "# Here are the first 10 words from the vocabulary:\n",
    "input_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlC4xuZnKLBS"
   },
   "outputs": [],
   "source": [
    "output_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct_custom,\n",
    "    max_tokens=max_vocab_size)\n",
    "\n",
    "output_text_processor.adapt(targ)\n",
    "output_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9KZxj8IrNZ9S"
   },
   "outputs": [],
   "source": [
    "example_tokens = input_text_processor(example_input_batch)\n",
    "example_tokens[:3, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98g9rcxGQY0I"
   },
   "outputs": [],
   "source": [
    "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
    "tokens = input_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzQWx2saImMV"
   },
   "source": [
    "Before getting into it define a few constants for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_a9uNz3-IrF-"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
    "                                                   embedding_dim)\n",
    "\n",
    "        # The GRU RNN layer processes those vectors sequentially.\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       # Return the sequence and state\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, tokens, state=None):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(tokens, ('batch', 's'))\n",
    "\n",
    "        # 2. The embedding layer looks up the embedding for each token.\n",
    "        vectors = self.embedding(tokens)\n",
    "        shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
    "\n",
    "        # 3. The GRU processes the embedding sequence.\n",
    "        #    output shape: (batch, s, enc_units)\n",
    "        #    state shape: (batch, enc_units)\n",
    "        output, state = self.gru(vectors, initial_state=state)\n",
    "        shape_checker(output, ('batch', 's', 'enc_units'))\n",
    "        shape_checker(state, ('batch', 'enc_units'))\n",
    "\n",
    "        # 4. Returns the new sequence and its state.\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3SKkaQeGn-Q"
   },
   "source": [
    "Here is how it fits together so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60gSVh05Jl6l"
   },
   "outputs": [],
   "source": [
    "# Convert the input text to tokens.\n",
    "example_tokens = input_text_processor(example_input_batch)\n",
    "\n",
    "# Encode the input sequence.\n",
    "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)\n",
    "example_enc_output, example_enc_state = encoder(example_tokens)\n",
    "\n",
    "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
    "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
    "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "momiE59lXo6U"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # For Eqn. (4), the  Bahdanau attention\n",
    "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "    def call(self, query, value, mask):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(query, ('batch', 't', 'query_units'))\n",
    "        shape_checker(value, ('batch', 's', 'value_units'))\n",
    "        shape_checker(mask, ('batch', 's'))\n",
    "\n",
    "        # From Eqn. (4), `W1@ht`.\n",
    "        w1_query = self.W1(query)\n",
    "        shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
    "\n",
    "        # From Eqn. (4), `W2@hs`.\n",
    "        w2_key = self.W2(value)\n",
    "        shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
    "\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs = [w1_query, value, w2_key],\n",
    "            mask=[query_mask, value_mask],\n",
    "            return_attention_scores = True,\n",
    "        )\n",
    "        shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
    "        shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4QMlOp8Gidh"
   },
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7y7hjPkNMmHh"
   },
   "outputs": [],
   "source": [
    "# Later, the decoder will generate this attention query\n",
    "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
    "\n",
    "# Attend to the encoded tokens\n",
    "\n",
    "context_vector, attention_weights = attention_layer(\n",
    "    query=example_attention_query,\n",
    "    value=example_enc_output,\n",
    "    mask=(example_tokens != 0))\n",
    "\n",
    "print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n",
    "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuzrCdmYlTcJ"
   },
   "outputs": [],
   "source": [
    "# attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIMwC-f-ZC8N"
   },
   "outputs": [],
   "source": [
    "attention_slice = attention_weights[0, 0].numpy()\n",
    "attention_slice = attention_slice[attention_slice != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erYvHIgAl8kh"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # For Step 1. The embedding layer convets token IDs to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
    "                                                   embedding_dim)\n",
    "\n",
    "        # For Step 2. The RNN keeps track of what's been generated so far.\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        # For step 3. The RNN output will be the query for the attention layer.\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "        # For step 4. Eqn. (3): converting `ct` to `at`\n",
    "        self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
    "                                        use_bias=False)\n",
    "\n",
    "        # For step 5. This fully connected layer produces the logits for each\n",
    "        # output token.\n",
    "        self.fc = tf.keras.layers.Dense(self.output_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WfSIb2sArRT"
   },
   "outputs": [],
   "source": [
    "class DecoderInput(typing.NamedTuple):\n",
    "    new_tokens: Any\n",
    "    enc_output: Any\n",
    "    mask: Any\n",
    "\n",
    "class DecoderOutput(typing.NamedTuple):\n",
    "    logits: Any\n",
    "    attention_weights: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJOi5btHAPNK"
   },
   "outputs": [],
   "source": [
    "def call(self,\n",
    "         inputs: DecoderInput,\n",
    "         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(inputs.new_tokens, ('batch', 't'))\n",
    "    shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
    "    shape_checker(inputs.mask, ('batch', 's'))\n",
    "\n",
    "    if state is not None:\n",
    "        shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "    # Step 1. Lookup the embeddings\n",
    "    vectors = self.embedding(inputs.new_tokens)\n",
    "    shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
    "\n",
    "    # Step 2. Process one step with the RNN\n",
    "    rnn_output, state = self.gru(vectors, initial_state=state)\n",
    "\n",
    "    shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
    "    shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "    # Step 3. Use the RNN output as the query for the attention over the\n",
    "    # encoder output.\n",
    "    context_vector, attention_weights = self.attention(\n",
    "      query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
    "    shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
    "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "\n",
    "    # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
    "    #     [ct; ht] shape: (batch t, value_units + query_units)\n",
    "    context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "\n",
    "    # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
    "    attention_vector = self.Wc(context_and_rnn_output)\n",
    "    shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
    "\n",
    "    # Step 5. Generate logit predictions:\n",
    "    logits = self.fc(attention_vector)\n",
    "    shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
    "\n",
    "    return DecoderOutput(logits, attention_weights), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ay_mTMPfnb2a"
   },
   "outputs": [],
   "source": [
    "Decoder.call = call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZUMbYXIEVeA"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4u6eJBU4GL40"
   },
   "outputs": [],
   "source": [
    "# Convert the target sequence, and collect the \"[START]\" tokens\n",
    "example_output_tokens = output_text_processor(example_target_batch)\n",
    "\n",
    "start_index = output_text_processor.get_vocabulary().index('[START]')\n",
    "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5hqvbR5FUCD"
   },
   "outputs": [],
   "source": [
    "# Run the decoder\n",
    "dec_result, dec_state = decoder(\n",
    "    inputs = DecoderInput(new_tokens=first_token,\n",
    "                          enc_output=example_enc_output,\n",
    "                          mask=(example_tokens != 0)),\n",
    "    state = example_enc_state\n",
    ")\n",
    "\n",
    "print(f'logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\n",
    "print(f'state shape: (batch_size, dec_units) {dec_state.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5UY8wko3jFp"
   },
   "outputs": [],
   "source": [
    "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKXTLYu4IV7I"
   },
   "outputs": [],
   "source": [
    "vocab = np.array(output_text_processor.get_vocabulary())\n",
    "first_word = vocab[sampled_token.numpy()]\n",
    "# first_word[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pX1VF9XDJTOM"
   },
   "outputs": [],
   "source": [
    "dec_result, dec_state = decoder(\n",
    "    DecoderInput(sampled_token,\n",
    "                 example_enc_output,\n",
    "                 mask=(example_tokens != 0)),\n",
    "    state=dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1rs0XL7Y2aS"
   },
   "outputs": [],
   "source": [
    "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\n",
    "first_word = vocab[sampled_token.numpy()]\n",
    "first_word[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        self.name = 'masked_loss'\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(y_true, ('batch', 't'))\n",
    "        shape_checker(y_pred, ('batch', 't', 'logits'))\n",
    "\n",
    "        # Calculate the loss for each item in the batch.\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        shape_checker(loss, ('batch', 't'))\n",
    "\n",
    "        # Mask off the losses on padding.\n",
    "        mask = tf.cast(y_true != 0, tf.float32)\n",
    "        shape_checker(mask, ('batch', 't'))\n",
    "        loss *= mask\n",
    "\n",
    "        # Return the total.\n",
    "        return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWIyuy71TkJT"
   },
   "outputs": [],
   "source": [
    "class TrainTranslator(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units,\n",
    "               input_text_processor,\n",
    "               output_text_processor, \n",
    "               use_tf_function=True):\n",
    "        super().__init__()\n",
    "        # Build the encoder and decoder\n",
    "        encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                          embedding_dim, units)\n",
    "        decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                          embedding_dim, units)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "        self.use_tf_function = use_tf_function\n",
    "        self.shape_checker = ShapeChecker()\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        self.shape_checker = ShapeChecker()\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "            return self._train_step(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlYE68wzXoA8"
   },
   "outputs": [],
   "source": [
    "def _preprocess(self, input_text, target_text):\n",
    "    self.shape_checker(input_text, ('batch',))\n",
    "    self.shape_checker(target_text, ('batch',))\n",
    "\n",
    "    # Convert the text to token IDs\n",
    "    input_tokens = self.input_text_processor(input_text)\n",
    "    target_tokens = self.output_text_processor(target_text)\n",
    "    self.shape_checker(input_tokens, ('batch', 's'))\n",
    "    self.shape_checker(target_tokens, ('batch', 't'))\n",
    "\n",
    "    # Convert IDs to masks.\n",
    "    input_mask = input_tokens != 0\n",
    "    self.shape_checker(input_mask, ('batch', 's'))\n",
    "\n",
    "    target_mask = target_tokens != 0\n",
    "    self.shape_checker(target_mask, ('batch', 't'))\n",
    "\n",
    "    return input_tokens, input_mask, target_tokens, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHy6hzStrgjQ"
   },
   "outputs": [],
   "source": [
    "TrainTranslator._preprocess = _preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qs_gsISsYPpY"
   },
   "outputs": [],
   "source": [
    "def _train_step(self, inputs):\n",
    "    input_text, target_text = inputs  \n",
    "\n",
    "    (input_tokens, input_mask,\n",
    "    target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
    "\n",
    "    max_target_length = tf.shape(target_tokens)[1]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Encode the input\n",
    "        enc_output, enc_state = self.encoder(input_tokens)\n",
    "        self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
    "        self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
    "\n",
    "        # Initialize the decoder's state to the encoder's final state.\n",
    "        # This only works if the encoder and decoder have the same number of\n",
    "        # units.\n",
    "        dec_state = enc_state\n",
    "        loss = tf.constant(0.0)\n",
    "\n",
    "    for t in tf.range(max_target_length-1):\n",
    "        # Pass in two tokens from the target sequence:\n",
    "        # 1. The current input to the decoder.\n",
    "        # 2. The target for the decoder's next prediction.\n",
    "        new_tokens = target_tokens[:, t:t+2]\n",
    "        step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
    "                                             enc_output, dec_state)\n",
    "        loss = loss + step_loss\n",
    "\n",
    "    # Average the loss over all non padding tokens.\n",
    "    average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
    "\n",
    "    # Apply an optimization step\n",
    "    variables = self.trainable_variables \n",
    "    gradients = tape.gradient(average_loss, variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    # Return a dict mapping metric names to current value\n",
    "    return {'batch_loss': average_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGwWHIxLrjGR"
   },
   "outputs": [],
   "source": [
    "TrainTranslator._train_step = _train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VrzgwztXzYJ"
   },
   "outputs": [],
   "source": [
    "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
    "    input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
    "\n",
    "    # Run the decoder one step.\n",
    "    decoder_input = DecoderInput(new_tokens=input_token,\n",
    "                               enc_output=enc_output,\n",
    "                               mask=input_mask)\n",
    "\n",
    "    dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
    "    self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n",
    "    self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n",
    "    self.shape_checker(dec_state, ('batch', 'dec_units'))\n",
    "\n",
    "    # `self.loss` returns the total for non-padded tokens\n",
    "    y = target_token\n",
    "    y_pred = dec_result.logits\n",
    "    step_loss = self.loss(y, y_pred)\n",
    "\n",
    "    return step_loss, dec_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xj3I7VULrk1R"
   },
   "outputs": [],
   "source": [
    "TrainTranslator._loop_step = _loop_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OA6bCske8TXm"
   },
   "outputs": [],
   "source": [
    "translator = TrainTranslator(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    "    use_tf_function=False)\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "translator.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHe-OudqCFGK"
   },
   "outputs": [],
   "source": [
    "np.log(output_text_processor.vocabulary_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFUsTKQx0jaH"
   },
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
    "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
    "def _tf_train_step(self, inputs):\n",
    "    return self._train_step(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-bgU59jrztQ"
   },
   "outputs": [],
   "source": [
    "TrainTranslator._tf_train_step = _tf_train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KC8bRv_Gr3H9"
   },
   "outputs": [],
   "source": [
    "translator.use_tf_function = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLQZsX2dp1QK"
   },
   "outputs": [],
   "source": [
    "translator.train_step([example_input_batch, example_target_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-dIWMIBqK7b"
   },
   "outputs": [],
   "source": [
    "# losses = []\n",
    "# for n in range(100):\n",
    "#   print('.', end='')\n",
    "#   logs = translator.train_step([example_input_batch, example_target_batch])\n",
    "#   losses.append(logs['batch_loss'].numpy())\n",
    "\n",
    "# print()\n",
    "# plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Emgfgh4tAmJt"
   },
   "outputs": [],
   "source": [
    "train_translator = TrainTranslator(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor)\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "train_translator.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7m4mtnj80sq"
   },
   "outputs": [],
   "source": [
    "class BatchLogs(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.logs = []\n",
    "\n",
    "    def on_train_batch_end(self, n, logs):\n",
    "        self.logs.append(logs[self.key])\n",
    "\n",
    "batch_loss = BatchLogs('batch_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BQd_esVVoSf3",
    "outputId": "b098272c-564e-4d56-dcc4-9c13d5c812aa"
   },
   "outputs": [],
   "source": [
    "train_translator.fit(dataset, epochs=30,\n",
    "                     callbacks=[batch_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PO-CLL1LVBbM"
   },
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, input_text_processor,\n",
    "               output_text_processor):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "\n",
    "        self.output_token_string_from_index = (\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=output_text_processor.get_vocabulary(),\n",
    "                mask_token='',\n",
    "                invert=True))\n",
    "\n",
    "        # The output should never generate padding, unknown, or start.\n",
    "        index_from_string = tf.keras.layers.StringLookup(\n",
    "            vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
    "        token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
    "\n",
    "        token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
    "        token_mask[np.array(token_mask_ids)] = True\n",
    "        self.token_mask = token_mask\n",
    "\n",
    "        self.start_token = index_from_string(tf.constant('[START]'))\n",
    "        self.end_token = index_from_string(tf.constant('[END]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBQzFZ9uWU79"
   },
   "outputs": [],
   "source": [
    "translator = Translator(\n",
    "    encoder=train_translator.encoder,\n",
    "    decoder=train_translator.decoder,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IjwKTwtmdFf"
   },
   "outputs": [],
   "source": [
    "def tokens_to_text(self, result_tokens):\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(result_tokens, ('batch', 't'))\n",
    "    result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
    "    shape_checker(result_text_tokens, ('batch', 't'))\n",
    "\n",
    "    result_text = tf.strings.reduce_join(result_text_tokens,\n",
    "                                       axis=1, separator=' ')\n",
    "    shape_checker(result_text, ('batch'))\n",
    "\n",
    "    result_text = tf.strings.strip(result_text)\n",
    "    shape_checker(result_text, ('batch',))\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "912aV0K7r90w"
   },
   "outputs": [],
   "source": [
    "Translator.tokens_to_text = tokens_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWCMHdoS32QN"
   },
   "outputs": [],
   "source": [
    "example_output_tokens = tf.random.uniform(\n",
    "    shape=[5, 2], minval=0, dtype=tf.int64,\n",
    "    maxval=output_text_processor.vocabulary_size())\n",
    "translator.tokens_to_text(example_output_tokens).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lfuj3GcdD6e"
   },
   "outputs": [],
   "source": [
    "def sample(self, logits, temperature):\n",
    "    shape_checker = ShapeChecker()\n",
    "    # 't' is usually 1 here.\n",
    "    shape_checker(logits, ('batch', 't', 'vocab'))\n",
    "    shape_checker(self.token_mask, ('vocab',))\n",
    "\n",
    "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
    "    shape_checker(token_mask, ('batch', 't', 'vocab'), broadcast=True)\n",
    "\n",
    "    # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
    "    logits = tf.where(self.token_mask, -np.inf, logits)\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        new_tokens = tf.argmax(logits, axis=-1)\n",
    "    else: \n",
    "        logits = tf.squeeze(logits, axis=1)\n",
    "        new_tokens = tf.random.categorical(logits/temperature,\n",
    "                                        num_samples=1)\n",
    "  \n",
    "    shape_checker(new_tokens, ('batch', 't'))\n",
    "\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DpDnBdBdL9_"
   },
   "outputs": [],
   "source": [
    "Translator.sample = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwLT0nxXym80"
   },
   "outputs": [],
   "source": [
    "example_logits = tf.random.normal([5, 1, output_text_processor.vocabulary_size()])\n",
    "example_output_tokens = translator.sample(example_logits, temperature=1.0)\n",
    "# example_output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmOvVrZmwAxg"
   },
   "outputs": [],
   "source": [
    "def translate_unrolled(self,\n",
    "                       input_text, *,\n",
    "                       max_length=50,\n",
    "                       return_attention=True,\n",
    "                       temperature=1.0):\n",
    "    batch_size = tf.shape(input_text)[0]\n",
    "    input_tokens = self.input_text_processor(input_text)\n",
    "    enc_output, enc_state = self.encoder(input_tokens)\n",
    "\n",
    "    dec_state = enc_state\n",
    "    new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "\n",
    "    result_tokens = []\n",
    "    attention = []\n",
    "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        dec_input = DecoderInput(new_tokens=new_tokens,\n",
    "                                 enc_output=enc_output,\n",
    "                                 mask=(input_tokens!=0))\n",
    "\n",
    "        dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
    "\n",
    "        attention.append(dec_result.attention_weights)\n",
    "\n",
    "        new_tokens = self.sample(dec_result.logits, temperature)\n",
    "\n",
    "        # If a sequence produces an `end_token`, set it `done`\n",
    "        done = done | (new_tokens == self.end_token)\n",
    "        # Once a sequence is done it only produces 0-padding.\n",
    "        new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
    "\n",
    "        # Collect the generated tokens\n",
    "        result_tokens.append(new_tokens)\n",
    "\n",
    "        if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "            break\n",
    "\n",
    "    # Convert the list of generates token ids to a list of strings.\n",
    "    result_tokens = tf.concat(result_tokens, axis=-1)\n",
    "    result_text = self.tokens_to_text(result_tokens)\n",
    "\n",
    "    if return_attention:\n",
    "        attention_stack = tf.concat(attention, axis=1)\n",
    "        return {'text': result_text, 'attention': attention_stack}\n",
    "    else:\n",
    "        return {'text': result_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOmd8Y269MG3"
   },
   "outputs": [],
   "source": [
    "Translator.translate = translate_unrolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hd2rgyHwVVrv"
   },
   "outputs": [],
   "source": [
    "input_text = tf.constant([\n",
    "    'The bride and groom danced their last dance before the wedding ended with their own choice of a song.'\n",
    "    'Who fell in love with Bill?'\n",
    "])\n",
    "\n",
    "result = translator.translate(\n",
    "    input_text = input_text)\n",
    "\n",
    "# print(result['text'][0].numpy().decode())\n",
    "# print(result['text'][1].numpy().decode())\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXaE-zY6kcIC"
   },
   "outputs": [],
   "source": [
    "with open(\"sentencesTest.txt\", encoding = \"unicode_escape\" ) as f:\n",
    "    all_sentences = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "with open(\"tokensTestPredicted.txt\", \"w\" ) as pred:\n",
    "    for cur_sent in all_sentences:\n",
    "        temp = []\n",
    "        temp.append(cur_sent)\n",
    "        text = tf.constant(temp)\n",
    "        result = translator.translate(text)\n",
    "        \n",
    "        translated = result['text'][0].numpy().decode().replace(\" \", \",\").upper()\n",
    "        pred.write(translated + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### References: https://github.com/tensorflow/text/blob/master/docs/tutorials/nmt_with_attention.ipynb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Updated nmt_with_attention.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
